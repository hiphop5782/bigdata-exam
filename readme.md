제공된 YouTube 영상 강의 내용을 바탕으로 빅데이터 분석기사 필기 요약 내용을 정리해 드립니다. 강의는 4과목으로 구성되어 있으며, 각 과목의 핵심 내용을 중심으로 설명합니다.

---

### **[1과목] 빅데이터 분석 기획**

*   **빅데이터 개요**:
    *   **데이터의 정의**: 가공되지 않은 객관적 사실 [1].
    *   **빅데이터 출현 배경**: 인터넷 확산, 스마트폰 보급, 클라우드 컴퓨팅(대용량 컴퓨팅 가능, 경제성 확보), 저장 매체 가격 하락, 하둡(분산 컴퓨팅, 처리 속도 향상) 등 [1].
    *   **빅데이터 활용 3대 요소**: **인**력, **자**원(데이터), 기**술** (인자이) [1].
    *   **빅데이터의 3V**: 가트너(Gartner)가 정의한 **규모(Volume), 다양성(Variety), 속도(Velocity)** [1]. 이후 가치(Value)와 신뢰성(Veracity)이 추가되어 5V로 확장 [1].
*   **DIKW 피라미드**: 데이터, 정보, 지식, 지혜의 계층적 관계 [2].
    *   **데이터**: 객관적 사실 (예: A대리점 100만원, B대리점 200만원) [2].
    *   **정보**: 가공된 데이터 (예: A대리점이 B대리점보다 싸다는 패턴 인식) [2].
    *   **지식**: 예측 가능 (예: A대리점 폰을 사면 이득을 본다) [2].
    *   **지혜**: 창의적 통찰 (예: A대리점의 다른 기기도 저렴할까) [2].
*   **암묵지 및 형식지**:
    *   **암묵지**: 겉으로 드러나지 않는 지식 (예: 요리사의 레시피를 본인만 소유) [2].
    *   **형식지**: 문서나 매뉴얼 등으로 형상화된 지식 (예: 요리 블로그, 요리책) [2].
    *   **지식 변환 과정**: **공**동화(암묵지→암묵지), **표**출화(암묵지→형식지), **연**결화(형식지→형식지), **내**면화(형식지→암묵지) (공표 연내) [2].
*   **데이터베이스 (DB)**: 데이터의 집합 [2].
    *   **구성 요소**: **스키마**(DB 설명), **인스턴스**(데이터 타입과 값), **메타데이터**(데이터를 설명하는 데이터), **인덱스**(정렬 탐색용 이름) [2].
    *   **접근 방식**: **DBMS**(데이터베이스 관리 시스템)를 통해 접근 [2].
        *   **관계형 DBMS (RDBMS)**: 전통적, 테이블 형태로 정리, 형식이 정해져 있음 [2].
        *   **NoSQL DBMS**: 빅데이터 시대에 등장, 비정형 데이터(SNS 게시물, 동영상 등) 저장 및 관리 [3].
    *   **언어**: **SQL**(데이터 정의/조작/제어 언어) [3].
    *   **특징**: **공**용, **통**합, **저**장, **변**화 (공통 저변) [3].
*   **빅데이터가 만들어내는 변화**:
    *   **전후 양상**: **전**수조사 (표본조사 → 전수조사), **사후**처리 (사전처리 → 사후처리), **상관**관계 (원인결과 → 상관관계) [3]. 이 부분은 시험 단골 출제 [3].
*   **데이터 산업 발전**: 프로그램 언어 → DB → 분석 기술 → 모듈 연결 → **마이데이터** (데이터 주권) → **데이터 사이언스** 등장 [3, 4].
*   **데이터 사이언스 구성 요소**: AI 지식, R 지식, 비즈니스적 지식 [3].
    *   **데이터 사이언티스트**: 하드 스킬(이과적)과 소프트 스킬(문과적) 모두 필요 [4].
*   **하둡 (Hadoop)**: 오픈 소스 빅데이터 솔루션 [4].
    *   **주요 코어 프로젝트**: **HDFS**(분산 파일 시스템), **맵리듀스**(분산 데이터 병렬 처리) [4].
*   **데이터 단위**: 페타바이트(PB), 엑사바이트(EB), 제타바이트(ZB), 요타바이트(YB) (외우는 방법: 폐지 엑제 요) [4].
*   **빅데이터 조직 구조**:
    *   **집중 구조**: 독립적인 전담 조직 구성 [4].
    *   **기능 구조**: 해당 부서에서 직접 분석 [4].
    *   **분산 구조**: 분석 조직 인력을 현업 부서에 배치 [4].
*   **빅데이터 플랫폼**: 빅데이터 분석 파이프라인 전 과정을 통합적으로 제공하는 환경 [4].
    *   **계층**: 소프트웨어 계층, 플랫폼 계층, 인프라스트럭처 계층 [4].
*   **인공지능(AI), 머신러닝(ML), 딥러닝(DL) 관계**: AI(사람 지능 구현) ⊃ ML(사람이 모델 정의, 추론) ⊃ DL(인공신경망, 뇌 모방) [4].
    *   **머신러닝 학습 기법**: 지도 학습(정답 O), 비지도 학습(정답 X), 준지도 학습(혼합), 강화 학습(보상 기반) [4, 5].
*   **개인정보 법제도**: **데이터 3법** (개인정보보호법, 정보통신망법, 신용정보법) [5].
    *   **가명정보 개념 도입**: 동의 없이 활용 가능 [5].
    *   **개인정보**: 개인 식별 가능, 동의 필요 [5].
    *   **가명정보**: 가명 처리, 특정 목적 하에 동의 없이 활용 가능 [5].
    *   **익명정보**: 개인 식별 불가, 제한 없이 자유롭게 활용 가능 [5].
    *   **개인정보 비식별 조치**: 검토 → 비식별 조치 → 적정성 평가 → 사후 관리 [5].
*   **분석 문제 정의**:
    *   **하향식 접근**: 문제가 주어지면 위에서 아래로 접근 (What/How, 솔루션, 비즈니스 모델 캔버스) [6].
    *   **상향식 접근**: 문제 정의가 안 된 경우 아래에서 위로 접근 (What 관점, 비지도 학습 주로 활용) [6].
    *   **혼합 접근**: 하향식과 상향식 결합 [6].
*   **데이터 분석 방법론 구성 요소**: 절차, 방법, 도구/기법, 템플릿, 산출물 [6, 7].
*   **분석 고려 5가지 요소**: 크기, 속도, 복잡도, **정확도, 정밀도** (트레이드오프 관계) [7].
*   **PMBOK 10가지 영역**: 통합, 범위, 통신, 이해관계자, 자원, 시간, 품질, 리스크, 조달, 원가 (이범 통이 의자 시원 조립품) [7].
*   **분석 방법론 모델**:
    *   계층적 프로세스 모델, 폭포수 모델(탑다운), 나선형 모델(점진적 완성), 프로토타입 모델(일부 시연), 반복적 모델, **애자일 모델**(짧은 개발 주기, 지속적 피드백) [7].
*   **CRISP-DM 분석 방법론**:
    *   **순서**: **업**무 이해, **데**이터 이해, **데**이터 준비, **모**델링, **평**가, **전**개 (업데 데이트 모델 평가전) [7].
    *   모델링 단계에서 모델 작성 및 평가를 함께 수행 [7].
*   **빅데이터 분석 방법론**: PPAD-DD (시험에 매우 중요) [7].
    *   **위험 계획 수립**: **회**피, **전**가, **완**화, **수**용 (회전 완수) [7].
*   **분석 거버넌스 체계 구성 요소**: **시**스템, **조**직, **프로**세스, **마**인드(인력 육성), **데**이터 (시조 프로마인드 대) [7].
*   **데이터 분석 수준 진단**:
    *   **분석 준비도**: IT, 인프라, 데이터, 조직, 문화 (IT 문대기 인파) [7, 8].
    *   **분석 성숙도**: CMMI 모델 기반, **도**입, **활**용, **확**산, **최**적화 (도활 확재) [8].
*   **분석 수준 진단 매트릭스 (준비도-성숙도)**: **도**입형, **준**비형, **정**착형, **확**산형 (도준 정학, 시계 방향) [8]. 확산형이 가장 높은 수준 [8].
*   **분석 지원 인프라 방안**: 플랫폼 구조 적용 (중앙 집중 관리) [8].
    *   **분석 플랫폼 구성 요소**: 협의의 플랫폼(데이터 처리 프레임워크, 분석 엔진, 라이브러리), 광의의 플랫폼(제공 엔진, API, 하드웨어, OS) [8].
*   **데이터 거버넌스**: 전사 차원 표준화된 관리 체계 수립 [8].
    *   **구성 요소**: 원칙, 조직, 프로세스 (원조) [8].
    *   **관리 대상**: 데이터 표준화, 관리 체계, 저장소, 표준화 활동 [8].
    *   **빅데이터 거버넌스**: 기존 데이터 거버넌스에 빅데이터 관련 요소(효율적 관리, 최적화, 정보보호) 추가 [8].
*   **데이터 수집 및 저장 계획**:
    *   **수집 기술**: **ETL**(추출, 변환, 적재), FTP, API, Apache Sqoop/Flume, 크롤링 [8].
    *   **데이터 유형**: 정성적/정량적 [8].
        *   **정형 데이터**: 테이블 형태, 프레임 정해짐 (관계형 DB) [8].
        *   **반정형 데이터**: 프레임은 없으나 메타데이터 포함 (XML, JSON, HTML 등) [8].
        *   **비정형 데이터**: 형식이 정해지지 않음 (SNS 글, 영상, 음원) [8].
    *   **데이터 척도**:
        *   **질적 척도**: 명목 척도(집단 분류), 순서 척도(서열 관계) [9].
        *   **양적 척도**: 등간 척도(간격 의미, 덧셈/뺄셈), 비율 척도(절대 0 존재, 사칙연산 가능) [9].
*   **개인정보 비식별화**: 가명 처리 기법, 총계 처리 기법 등 [9].
*   **프라이버시 보호 모델**: **K-익명성, L-다양성, T-근접성** (보호력 점진적 상승) [9].
    *   **차등 정보보호**: 노이즈 추가를 통한 개인정보 침해 방지 [9].
*   **데이터 품질 기준 및 진단/개선 절차** [9].
*   **데이터 저장**:
    *   **분산 파일 시스템**: **HDFS**(하둡 핵심, 마스터/슬레이브 노드, 분산 저장), GFS [9, 10].
    *   **데이터베이스**: RDBMS, NoSQL DB (다양한 유형 존재) [10].
    *   **병렬 DBMS**: 병렬 처리를 통해 성능 개선 (공유 메모리, 공유 디스크, 공유 없음 구조) [10].
*   **데이터 웨어하우스 (DWH)**: 기업 내 다양한 DB 통합 [10].
    *   **4가지 특성**: **분석 목적** 설정, **통합**됨, **시계열성** (히스토리), **비휘발성** (읽기 전용) [10].
    *   **구성 요소**: ETL → ODS (임시 저장) → DWH [10].
*   **데이터 마트**: DWH보다 소규모, 특정 목적/부서 사용 [10].
*   **데이터 레이크**: 비정형 데이터를 호수처럼 저장 [10].

---

### **[2과목] 빅데이터 탐색**

*   **데이터 전처리**:
    *   **데이터 종류**:
        *   단변량 데이터(특성 1개), 다변량 데이터(특성 2개 이상) [11].
        *   종단면적 데이터(여러 시점), 횡단면적 데이터(한 시점), 패널 데이터(종단+횡단) [11].
    *   **데이터 정제**: 요약, 일반화, 정규화, 평활화(잡음 제거, 시계열 데이터) [11].
    *   **결측값 처리**:
        *   **결측값 종류**: 완전 무작위 결측(MCAR), 무작위 결측(MAR), 비무작위 결측(MNAR) [11].
        *   **처리 방법**: 완전 분석법(삭제), 평균 대치법, 회귀 대치법, 단순 확률 대치법, K-NN 대치법, 핫덱/콜드덱, 다중 대치법 [12]. 결측값은 마음대로 처리하면 신뢰도 저하 [12].
    *   **이상값 처리**:
        *   **이상값**: 극단적으로 크거나 작은 값 [12]. **항상 제거하는 것은 위험** (의미 있는 데이터일 수 있음) [12].
        *   **처리 방법**: ESD 방법(평균으로부터 3표준편차 초과), **4분위수 방법 (IQR)** 및 **박스플롯** (Q1-1.5IQR ~ Q3+1.5IQR 범위 벗어남), Z-스코어, DBSCAN [12].
        *   **박스플롯 해석**: 데이터 분포, 중앙값 위치, 이상치 존재 여부 확인 가능. **평균은 알 수 없음** [12].
*   **변수 선택**:
    *   **방법**: 전진 선택법(변수 하나씩 추가), 후진 제거법(변수 하나씩 제거), 단계별 선택법(혼합) [12, 13].
    *   **상관 계수 매트릭스 분석**: 변수 간 상관관계 확인, 높은 상관관계 변수는 제거 고려 [13].
*   **차원 축소**:
    *   **차원의 저주**: 차원(변수 개수)이 높아질수록 빈 공간 증가, 알고리즘 성능 저하, 데이터 수집량 증가 필요 [13].
    *   **효과**: 시각화 가능, 노이즈 제거, 데이터 압축, 성능 향상, 특징 추출, 계산 비용 절감 [13].
    *   **기법**:
        *   **선형**: **주성분 분석 (PCA)** (분산이 최대화되는 방향으로 축소, 시험에 가장 자주 출제), LDA, ICA, SVD, 요인분석 [13, 14].
        *   **비선형**: MDS, t-SNE, UMAP, 오토인코더 [14].
*   **파생 변수 생성**:
    *   **요약 변수**: 정보 종합, 재활용성 높음 [14].
    *   **파생 변수**: 의미 부여, 논리적 타당성 필요 [14].
    *   **생성 방법**: 특징 추출, 결합, 부가 정보 결합, 수학적 변환, **교호작용** (두 개 이상의 독립변수가 상호작용하여 종속변수에 미치는 영향) [14].
*   **데이터 변환**:
    *   **수치형 자료 변환**:
        *   **Z-스코어 정규화**: 평균 0, 표준편차 1 (표준정규분포로 변환) [14].
        *   **최소-최대 정규화**: 0과 1 사이로 변환 [14].
        *   **로그 변환**: 한쪽으로 치우친 데이터를 정규분포 형태로 변환 [14].
    *   **범주형 자료 변환**:
        *   **레이블 인코딩**: 정수로 변환 (크기 관계 문제 발생 가능) [15].
        *   **원-핫 인코딩**: 해당 컬럼만 1, 나머지는 0 (크기 관계 문제 해결, 용량 증가) [15].
        *   **타겟 인코딩**: 타겟 변수 평균값으로 변환 [15].
*   **불균형 데이터 처리**:
    *   **문제점**: 특정 클래스 데이터가 현저히 적은 경우 [15].
    *   **처리 방법**: 가중치 부여, 언더 샘플링(다수 클래스 축소), 오버 샘플링(소수 클래스 복사/생성) [15].
*   **데이터 탐색 (EDA)**: 데이터의 분포, 패턴 등을 통계적/시각적으로 파악 [15].
    *   **상관 분석**:
        *   단순 상관 분석(두 변수), 다중 상관 분석(세 개 이상), 편상관 관계 분석(다른 변수 통제 후 두 변수 관계) [15].
        *   **피어슨 상관 분석**: 선형 관계 크기 측정 [15].
        *   **스피어만 상관 분석**: 서열 척도, 순서형 변수에 용이, 단조 관계 측정 [15].
*   **기초 통계량**:
    *   **중심 경향**: 평균, 중앙값, 최빈값, 분위수, 절사평균 [15].
    *   **분산**: 분산, 표준편차, 범위, 사분위범위, 중앙값절대편차 [15].
    *   **공분산**: 두 변수의 상관 정도 (0: 무상관, >0: 양의 상관, <0: 음의 상관). **값 크기로 상관 강도 판단 어려움** (무한대 범위) [16].
    *   **상관 계수**: -1에서 1 사이 값, 상관 강도 및 방향 명확히 표현 [16]. **공분산이 0이라고 독립인 것은 아님** [16].
    *   **첨도 (Kurtosis)**: 분포의 뾰족한 정도 (정규분포 기준 3) [16].
    *   **왜도 (Skewness)**: 분포의 비대칭 정도 (0: 대칭, >0: 오른쪽 꼬리, <0: 왼쪽 꼬리) [16]. **평균, 중앙값, 최빈값 관계를 통해 왜도 파악** [16].
    *   **Summary 함수 해석**: 평균, 중앙값, 결측치, 클래스 불균형 등 파악 [16, 17].
*   **시공간 데이터 탐색**: 공간과 시간의 흐름 데이터를 결합 (예: 태풍 경로) [17].
*   **비정형 데이터 탐색 (자연어 처리)**:
    *   **전처리**: **토큰화**(의미 있는 단위 분리), **불용어 처리**(의미 없는 단어 제거), **정규화**(표현 다른 단어 통합), **어간 추출**(원형 찾기), **표제어 추출**(사전형 원형 변환) [17].
*   **기초 확률 용어**: 조건부 확률, 독립 사건, 배반 사건, 베이즈 정리 [18].
*   **확률 분포**:
    *   **이산 확률 분포**: 값을 셀 수 있음 (베르누이, 이항, 포아송 등) [19].
    *   **연속 확률 분포**: 값을 셀 수 없음 (**정규 분포** - 일상생활 데이터, **t 분포** - 정규 분포 유사하나 표본 작을 때 사용, 카이제곱, F 분포 등) [19].
    *   **기댓값 계산**: 이산/연속 확률 변수에 따라 합/적분으로 계산 [19].
*   **중심 극한 정리 (CLT)**:
    *   **핵심**: 모집단의 분포 형태와 상관없이, **표본의 크기가 충분히 크면 (보통 n ≥ 30)**, **표본 평균의 분포는 정규 분포를 따른다** [20].
    *   **의미**: 모집단 분포를 몰라도 표본을 통해 통계적 추론 가능 [20].
*   **통계적 추론**:
    *   **점 추정**: 모수(모집단 특성)를 하나의 값으로 추정 (예: 배달 30분 뒤 도착) [20].
        *   **추정량의 조건**: 불편성(편향 0), 효율성(분산 작음), 일치성(표본↑ 모수 접근), 충분성(정보 최대 반영) [20].
        *   **표본 분산 N-1**: 자유도 개념 또는 모집단 분산 보정 위함 [21].
    *   **구간 추정**: 모수를 특정 구간으로 추정 (예: 배달 20~50분 사이) [21].
        *   **신뢰 구간**: 특정 모수가 포함될 확률 구간 (일반적으로 95% 또는 99%) [21].
        *   **표준 정규 분포 (Z-분포)**: 모집단 분산을 알 때 사용 [21].
        *   **t 분포**: 모집단 분산을 모를 때 사용 (자유도 n-1) [22].
*   **가설 검정**:
    *   **목표**: 모집단에 대한 주장을 가설로 세우고, 표본 조사로 가설의 채택 여부 판정 [22].
    *   **주요 개념**:
        *   **귀무가설 (H0)**: 일반적으로 알려진 사실, 차이가 없다는 주장 (예: 삼겹살 1인분 200g) [22].
        *   **대립가설 (H1)**: 귀무가설과 반대되는 주장, 차이가 있다는 주장 (예: 삼겹살 1인분 200g 아니다) [22].
        *   **검정 통계량**: 가설 검정에 사용되는 통계량 [22].
        *   **유의 수준 (α)**: 1종 오류(귀무가설이 참인데 기각할 확률)를 범할 최대 허용 확률 (보통 0.05 또는 5%) [22].
        *   **p-값 (p-value)**: 관측된 검정 통계량보다 극단적인 결과가 나올 확률 [22].
        *   **기각역/채택역**: p-값이 유의 수준보다 작으면 귀무가설 기각, 크면 채택 [22].
    *   **검정 종류**:
        *   **양측 검정**: 대립가설이 '같지 않다' (양쪽 꼬리) [22].
        *   **단측 검정**: 대립가설이 '크다' 또는 '작다' (한쪽 꼬리) [23].
        *   **단일 표본**: 모집단이 하나 [23].
        *   **대응 표본**: 동일 모집단에서 두 시점 비교 [23].
        *   **독립 표본**: 서로 다른 두 모집단 비교 [23].
    *   **문제 풀이 방법**: 귀무/대립가설 설정 → 양측/단측 검정 확인 → 표본 유형 확인 → 검정 통계량 계산 → 기각/채택 판단 [23]. (Z-검정 vs T-검정 예시로 상세 설명) [23, 24].

---

### **[3과목] 빅데이터 모델링**

*   **분석 모형 설계**:
    *   **과대적합 (Overfitting)**: 모델이 훈련 데이터에 너무 지나치게 학습되어 일반화 능력이 떨어짐 [25]. (흔들림이 심함, 분산 높음, 편향 낮음) [26].
    *   **과소적합 (Underfitting)**: 모델이 훈련 데이터에 충분히 학습되지 않아 데이터 특성을 제대로 설명 못함 [25]. (직선적, 분산 낮음, 편향 높음) [26].
    *   '과적합'은 보통 과대적합을 의미 [25].
*   **데이터 분할**:
    *   **훈련용 데이터 (Training Set)**: 모델 학습 [25].
    *   **검증용 데이터 (Validation Set)**: 모델 성능 검증 및 하이퍼파라미터 튜닝 [25].
    *   **평가용 데이터 (Test Set)**: 최종 모델 성능 평가 [25].
    *   **목적**: 과대적합/과소적합 방지, 데이터 불균형 문제 해결 [25].
*   **회귀 분석**: 수치형 종속변수를 독립변수들의 조합으로 예측 [25].
    *   **최소 제곱법 (OLS)**: 잔차(예측값과 실제값의 차이) 제곱의 합이 최소가 되도록 회귀 계수 추정 [25, 27].
    *   **회귀 모형 평가**:
        *   **결정 계수 (R-squared)**: 모델의 설명력 (0~1) [27].
        *   **SSE (오차 제곱합)**, **SSR (회귀 제곱합)**, **SST (전체 제곱합)** [27]. (SSE가 작을수록 좋음) [27].
    *   **선형 분석 가정**: 선형성, 등분산성, 독립성, 정상성 (선분 정독) [27].
    *   **다중 공선성**: 독립변수들 간의 강한 상관관계로 모델 불안정 [27]. **VIF 값 (10 이상)**으로 판단 [27].
    *   **회귀 분석 종류**: 단순, 다중, 다항, 릿지/라쏘 회귀 (과적합 해소 규제, L2/L1 규제), 교호작용 포함 회귀 [27, 28].
    *   **변수 선택**: 전진, 후진, 단계별 선택법 (벌점 AIC, BIC 고려) [28].
*   **로지스틱 회귀**: **범주형 데이터 분류 모델** (성공/실패 등) [28].
    *   **오즈 (Odds)**: 실패 확률 분의 성공 확률 [28].
    *   **로짓 변환 (Logit transformation)**: 오즈에 자연로그를 취해 선형 관계로 변환 [28].
    *   **시그모이드 함수**: 로짓 변환된 값을 0과 1 사이 확률로 출력 [28].
*   **의사 결정 나무 (Decision Tree)**: 스무고개처럼 분할, 노드 내 동질성, 노드 간 이질성 [28].
    *   **분할 방법**: 범주형: 지니 지수, 엔트로피 지수; 수치형: 분산 감소 [28].
    *   **과적합 방지**: 정지 규칙, 가지치기 (Pruning) [28].
*   **인공 신경망 (ANN)**: 인간 뇌 구조 모방 [28].
    *   **퍼셉트론**: 신경망의 기본 단위, 다중 회귀 형태 [28].
    *   **다층 퍼셉트론 (MLP)**: 입력층, **은닉층(사용자 설정)**, 출력층으로 구성 [29].
    *   **활성 함수**: 비선형성 극복 (시그모이드, 하이퍼볼릭 탄젠트, **ReLU** - 기울기 소실 문제 극복) [29].
    *   **출력층 활성 함수**: 시그모이드(이진 분류), **소프트맥스**(다중 분류, 확률 총합 1) [29].
    *   **손실 함수**: 예측값과 실제값의 차이 측정 (평균 제곱 오차 - 회귀, 교차 엔트로피 - 분류) [29].
    *   **학습 방법**: **순전파** → 손실 계산 → **역전파** → **경사 하강법** (손실 최소화) [29, 30].
    *   **기울기 소실 문제**: 은닉층 많을수록 기울기 작아져 학습 어려움 [30].
    *   **과적합 방지**: 규제(릿지/라쏘), **드롭아웃**(일부 뉴런 비활성화), 조기 종료, 배치 정규화 [30].
*   **서포트 벡터 머신 (SVM)**: 두 집단 간 **마진**을 최대화하는 **초평면 (하이퍼플레인)** 탐색 [30].
    *   **커널 함수 (Kernel Trick)**: 비선형 데이터를 고차원 공간으로 매핑하여 선형 분리 가능하게 함 [30].
*   **연관 분석 (Association Rule Mining)**: 장바구니 분석, 비목적성 분석 [30].
    *   **알고리즘**: Apriori 알고리즘 (빈발 항목 추출) [30].
    *   **평가 지표**: **지지도 (Support)**, **신뢰도 (Confidence)**, **향상도 (Lift)** [30, 31]. (향상도 1보다 크면 양의 상관 관계) [31].
*   **군집 분석 (Clustering)**: **비지도 학습**, 거리/유사성을 기준으로 군집 형성 [31].
    *   **거리 계산 방식**: 유클리드, 맨해튼 등 [31].
    *   **유사성 판단**: 자카드 유사도, 코사인 유사도 (범주형) [31].
    *   **계층적 군집 분석**: 군집 간 거리(최단/최장/평균/중심/와드 연결법)로 계층적 구조 형성, **덴드로그램**으로 시각화 [31].
    *   **K-평균 군집화 (K-Means Clustering)**: 비계층적, 거리 기반, K 값 사전 지정, 초기 중심점 설정에 민감, 이상치에 민감 [32].
    *   **DBSCAN**: 밀도 기반, 군집 개수 지정 불필요, 노이즈 및 이상치에 강함 [32].
*   **범주형 자료 분석**: 분할표 (상대 위험도, 오즈비) [32].
*   **K-최근접 이웃 (KNN)**: 분류 모델, 가장 가까운 K개의 이웃 분류 결과로 예측 [32].
    *   **특징**: K 값에 따라 결과 달라짐, **게으른 학습기 (Lazy Learner)** (별도 훈련 과정 없음) [33].
*   **시계열 분석**: 시간 흐름에 따른 자료 특성 파악 및 미래 예측 [33].
    *   **정상성**: 모든 시점에 평균과 분산이 일정해야 함 [33].
        *   **정상성 확보**: 차분(Differencing), 평활화 [33].
    *   **시계열 모형**: AR (자기 회귀), MA (이동 평균), **ARIMA (p, d, q)** - (d: 차분 횟수) [33].
    *   **분해 시계열**: 추세, 계절, 순환, 불규칙 요인으로 분리 [33].
*   **베이지안 기법**: 베이즈 정리 (사전 확률로 사후 확률 예측) [33].
    *   **나이브 베이즈 분류 모델**: 베이즈 정리 + 독립성 가정, 귀납적 추론 [34].
*   **딥러닝 분석**:
    *   **심층 신경망 (DNN)**: 은닉층이 2개 이상인 인공 신경망 [34].
    *   **합성곱 신경망 (CNN)**: 이미지 분석 특화 [34].
        *   **구조**: 컨볼루션 레이어(필터), 풀링 레이어(이미지 사이즈 축소), 플래튼, 완전 연결 레이어 [34].
        *   **활용**: 분류, 객체 탐지, 분할 [34].
    *   **순환 신경망 (RNN)**: 순차적 데이터(시계열, 자연어) 학습 특화 [34].
        *   **장기 의존성 문제**: 과거 정보 전달 어려움, LSTM/GRU로 극복 [34].
    *   **오토인코더**: 입력 데이터 압축(인코더) 및 복원(디코더) [34, 35].
        *   **활용**: 차원 축소, 데이터 생성 (생성형 AI 기반 - VAE, GAN), 이상 탐지 [35].
    *   **생성적 적대 신경망 (GAN)**: 생성기와 판별기의 경쟁을 통해 사실적인 데이터 생성 [35].
    *   **비정형 데이터 분석 (워드 임베딩)**: 컴퓨터가 문장을 처리하도록 변환 (원-핫 인코딩, Word2Vec 등) [35].
    *   **시퀀스-투-시퀀스 (Seq2Seq)**: RNN 기반, 한 시퀀스를 다른 시퀀스로 변환 (번역) [35].
    *   **트랜스포머 (Transformer)**: 어텐션 메커니즘 도입, 병렬 처리 및 정보 손실 개선 (BERT, GPT 기반) [35].
*   **앙상블 분석**: 여러 모형을 조합하여 성능 향상 [35].
    *   **보팅 (Voting)**: 여러 모델 결과 투표 [35].
    *   **배깅 (Bagging)**: 부트스트랩(복원 추출)으로 여러 데이터셋 생성, 각각 학습 후 투표 [35].
        *   **OOB (Out-of-Bag) 데이터**: 학습에 사용되지 않은 36.8% 데이터, 평가용으로 활용 가능 [36].
    *   **부스팅 (Boosting)**: 순차적으로 학습, 잘못 분류된 데이터에 가중치 부여 [36].
    *   **스태킹 (Stacking)**: 여러 모델의 예측 결과를 새로운 모델의 입력으로 사용하여 최종 예측 [36].
    *   **랜덤 포레스트**: 배깅 + 의사 결정 트리, 성능 우수, 이상치에 강함 [36].
    *   **병렬 처리 가능**: 보팅, 배깅, 랜덤 포레스트 [36].
    *   **병렬 처리 불가**: 부스팅 [36].
*   **모수 검정 vs 비모수 검정**:
    *   **모수 검정**: 모집단 분포를 가정한 검정 [36].
    *   **비모수 검정**: 모집단 분포 가정이 불가능할 때 순위나 차이로 검정 [36].

---

### **[4과목] 빅데이터 결과 해석**

*   **분류 모델 평가 지표**:
    *   **혼동 행렬 (Confusion Matrix)**: 예측값과 실제값을 교차하여 표시 (TP, TN, FP, FN) [37].
    *   **주요 지표**:
        *   **정확도 (Accuracy)**: 전체 중 올바르게 예측한 비율 [37].
        *   **정밀도 (Precision)**: 양성으로 예측한 것 중 실제 양성 비율 [37].
        *   **재현율 (Recall)/민감도 (Sensitivity)/TPR**: 실제 양성 중 올바르게 예측한 비율 [37].
        *   **F1-Score**: 정밀도와 재현율의 조화 평균 (**정밀도와 재현율은 트레이드오프 관계**) [37].
*   **ROC 커브 (Receiver Operating Characteristic Curve)**: TPR(재현율) vs FPR(1-특이도) 그래프, AUC(면적)가 1에 가까울수록 성능 우수 [38].
*   **회귀 모델 평가 지표**:
    *   **평균 제곱 오차 (MSE)**, **평균 절대 오차 (MAE)**, **제곱근 평균 제곱 오차 (RMSE)**, **평균 절대 백분율 오차 (MAPE)**: 모두 오차 크기 측정, 값이 낮을수록 좋음 [38].
    *   **결정 계수 (R-squared)**: 모델의 설명력 [38].
    *   **수정된 결정 계수 (Adjusted R-squared)**: 독립 변수가 많을 때 결정 계수의 왜곡 보정 [38].
*   **군집 분석 평가 지표**:
    *   **실루엣 계수 (Silhouette Coefficient)**: 자신이 속한 군집 응집도와 다른 군집과의 분리도 측정 (-1~1, 높을수록 좋음) [38].
    *   **WCSS (Within-Cluster Sum of Squares)**: 군집 내 데이터들이 중심값에 얼마나 모여있는지 측정 [38].
    *   **엘보 기법 (Elbow Method)**: WCSS를 활용하여 K-평균 군집화의 **최적 K(군집 개수)** 결정 (꺾이는 지점) [38].
*   **분석 모형 진단**:
    *   **과대적합/과소적합**: 분산-편향 트레이드오프 관계 [26].
    *   **교차 검증 (Cross-Validation)**: 과적합 방지 및 모델 성능 평가 [26].
        *   홀드아웃, **K-겹 교차 검증 (K-Fold CV)**, LOOCV, 부트스트래핑 [26].
*   **적합도 검정**: 데이터가 특정 분포를 따르는지 검정 (QQ-Plot 등) [26].
*   **모집단 유의성 분석 (가설 검정)**:
    *   Z-검정 (모집단 표준편차 알 때) [26].
    *   **T-검정**: **단일 표본, 대응 표본, 독립 표본** 평균 비교 [26, 39]. (R 결과 해석 예시) [39].
    *   F-검정 (분산 분석표): 회귀 모형의 유의성 분석 [40].
    *   **자유도**: 데이터 개수 - 1 [40, 41].
    *   **R 회귀 분석 결과 해석**: F 통계량(모형 유의성), t 검정(개별 회귀 계수 유의성), 결정 계수(설명력), 자유도, 교호작용 항 해석 [41, 42].
*   **비즈니스 기여도 평가**: 재무적, 성과적, 프로세스 측면 [42].
*   **시각화**: 분석 결과 보고/공표 시 활용 [42].
    *   시간, 공간, 관계/분포, 비교 시각화 등 [42, 43].
    *   **버블 차트**: 원의 크기가 면적으로 수량을 비교, 왜곡 가능성 [42].
    *   **트리맵**: 계층 구조 시각화, 음수 표현 어려움, 이웃 간 비교 용이, 멀리 떨어진 사각형 비교 어려움 [42].
*   **인포그래픽**: 정보 + 시각적 형상, **설득형 메시지 전달** (데이터 시각화: 정보형 메시지 전달) [42].
    *   **원리**: 단순, 명확, 중요, 일관성, 가독성, 효과적, 독자 맞춤, **오컴의 면도날** (복잡한 설명보다 단순한 설명 선호) [42, 43].
*   **분석 결과 활용**: 활용 계획 수립 → 시스템 구현 → 배포 → 피드백 및 개선 [43].
